{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"functions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datastore import DataStore\n",
    "from searchgrid import SearchGrid\n",
    "from crossvalidate import CrossValidate\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sampleddatastore import SampledDataStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load up our predefined functions for loading the data and running the model.\n",
    "\n",
    "Due to the high class imbalance, F1 score is a much better metric to use than just accuracy (since 99% of the data belongs to class 0). We will also have ROC-AUC for comparison.\n",
    "\n",
    "We fetch the true positives, false positives and false negatives to calculate the f1 score across all folds rather than using the builtin functionality. This is because the averaged f1 score returned by Sklearn is slighly biased for imbalanced class problems (for cross validation). This doesn't matter when evaluating the test set. All the relevant functions are in their respective python files (same folder as the notebook).\n",
    "\n",
    "Reference: https://www.hpl.hp.com/techreports/2009/HPL-2009-359.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load object for CrossValidation\n",
    "crossvalidate = CrossValidate()\n",
    "\n",
    "#Load object for GridSearchCV\n",
    "GridSpace = SearchGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish a baseline model that simply predicts the minority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.0038792557287250116\n",
      "ROC-AUC is: 0.5\n"
     ]
    }
   ],
   "source": [
    "classifier = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model is one that can perform better than the baseline, in terms of F1 Score. Anything below is worse than a model that simply predicts minority class.\n",
    "\n",
    "Note that 0.5 ROC-AUC score indicates that it's a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.14389101745423585\n",
      "ROC-AUC is: 0.5423767409646068\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's slightly better than a random classifier; this means that our model is learning some relationships for the underlying data, albeit small.\n",
    "\n",
    "The low score is to expected, especially given the class imbalance. Let's try using the class weight functionality that assigns weights to each class based on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.07312321528648821\n",
      "ROC-AUC is: 0.8275665348137995\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the balanced class weight performs worse in terms of f1 score (probably because it results in a lot more false positives).\n",
    "\n",
    "Let's test different parameters using GridSearchCV. We will be using our custom objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.29769820971867006\n",
      "ROC-AUC is: 0.6456231724605754\n",
      "Best Parameters: {'class_weight': {0: 1, 1: 10}}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:10,1:1}]}\n",
    "GridSpace.setGridParameters(parameters)\n",
    "GridSpace.setClassifier(LogisticRegression())\n",
    "GridSpace.run()\n",
    "parameters, scores = GridSpace.getMetrics().getBestResults()\n",
    "f1 = scores[0]\n",
    "roc = scores[1]\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making progress, but can we do even better?\n",
    "\n",
    "Adjusting the weights were not enough, we will have to try different sampling techniques. Imbalanced-learn library will come in handy here.\n",
    "\n",
    "We will start with RandomOverSampler to duplicates records from the minority class. We will use a sampling ratio of 0.1 (i.e. ~10% increase in gilded class).\n",
    "\n",
    "Read more: https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#a-practical-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampledDataStore = SampledDataStore()\n",
    "SampledDataStore.initializeSamplers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampling:\n",
      "F1 score is: 0.6781609747429389\n",
      "ROC-AUC is: 0.828094671658085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datastore.DataStore at 0x2aaaf0907fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using RandomOverSampler to duplicate records belonging to class 1 (gilded)\n",
    "random = SampledDataStore.getRandomSampled\n",
    "\n",
    "X_resampled, y_resampled = random()\n",
    "classifier = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "crossvalidate.getDataStore().setxTrain(X_resampled)\n",
    "crossvalidate.getDataStore().setyTrain(y_resampled)\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(\"Random Over Sampling:\")\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "\n",
    "crossvalidate.getDataStore()..revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate new samples with SMOTE and ADASYN based on existing samples. We will keep the sampling ratio the same for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE: \n",
      "F1 score is: 0.6686993132649306\n",
      "ROC-AUC is: 0.8241454838082516\n",
      "\n",
      "\n",
      "ADASYN: \n",
      "F1 score is: 0.5863412863698032\n",
      "ROC-AUC is: 0.802431136593601\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote = SampledDataStore.getSMOTESampled\n",
    "ada = SampledDataStore.getADASYNSampled\n",
    "samplers = [smote, ada]\n",
    "sampler_names = [\"SMOTE\", \"ADASYN\"]\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    crossvalidate.getDataStore().setxTrain(X_resampled)\n",
    "    crossvalidate.getDataStore().setyTrain(y_resampled)\n",
    "    classifier = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "    crossvalidate.setClassifier(classifier)\n",
    "    crossvalidate.run()\n",
    "    f1, roc = crossvalidate.getMetrics().getScores()\n",
    "    print(f\"{sampler_names[i]}: \")\n",
    "    print(f\"F1 score is: {f1}\")\n",
    "    print(f\"ROC-AUC is: {roc}\")\n",
    "    print(\"\\n\")\n",
    "        \n",
    "crossvalidate.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learn also recommends combining oversampling with undersampling the majority class.\n",
    "\n",
    "Ref: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html\n",
    "\n",
    "SMOTE can generate noisy samples (ex: when classes cannot be well separated), undersampling allows to clean the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE TOMEK: \n",
      "F1 score is: 0.6715279378987011\n",
      "ROC-AUC is: 0.8249707966279305\n",
      "\n",
      "\n",
      "SMOTE ENN: \n",
      "F1 score is: 0.7544018221916917\n",
      "ROC-AUC is: 0.8578015461438296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_tomek = SampledDataStore.getSMOTETOMEKSampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [smote_tomek, smote_enn]\n",
    "sampler_names = [\"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    crossvalidate.getDataStore().setxTrain(X_resampled)\n",
    "    crossvalidate.getDataStore().setyTrain(y_resampled)\n",
    "    classifier = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "    crossvalidate.setClassifier(classifier)\n",
    "    crossvalidate.run()\n",
    "    f1, roc = crossvalidate.getMetrics().getScores()\n",
    "    print(f\"{sampler_names[i]}: \")\n",
    "    print(f\"F1 score is: {f1}\")\n",
    "    print(f\"ROC-AUC is: {roc}\")\n",
    "    print(\"\\n\")\n",
    "        \n",
    "crossvalidate.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE, SMOTEENN and RandomOverSampler produces the best results so far. Let's evaluate those them on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OverSampler on test set:\n",
      "F1 score: 0.0638904734740445\n",
      "ROC_AUC score: 0.8183981729232407\n",
      "Balanced accuracy score: 0.8183981729232408\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.060296191819464044\n",
      "ROC_AUC score: 0.8228175600742684\n",
      "Balanced accuracy score: 0.8228175600742684\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.08897775556110973\n",
      "ROC_AUC score: 0.8434413510604898\n",
      "Balanced accuracy score: 0.8434413510604898\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random = SampledDataStore.getRandomSampled\n",
    "smote = SampledDataStore.getSMOTESampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [random, smote, smote_enn]\n",
    "sampler_names = [\"Random OverSampler\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    GridSpace.getDataStore().setxTrain(X_resampled)\n",
    "    GridSpace.getDataStore().setyTrain(y_resampled) \n",
    "    GridSpace.setGridParameters(parameters)\n",
    "    GridSpace.setClassifier(classifier)\n",
    "    grid = GridSpace.run()\n",
    "    y_preds = grid.predict(GridSpace.getDataStore().getxTest())\n",
    "    print(f\"{sampler_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "GridSpace.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression predicts the class probabilities for each sample and decides class based on a threshold (default: 0.5). We can also check if a different threshold value produces better results.\n",
    "\n",
    "Ref: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "\n",
    "Let's define the relevant functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def trainAndgetProbabilities(xTrain, yTrain, xTest):\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    model = LogisticRegressionCV(cv=rskf, class_weight=[{0: 1, 1: 10}])\n",
    "    model.fit(xTrain, yTrain)\n",
    "    return model.predict_proba(xTest)[:,1]\n",
    "\n",
    "def convert_probs(probs, threshold):\n",
    "    return (probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Oversampling on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.9470000000000001, F1 Score: 0.3143350604490501, ROC AUC: 0.681795495628806\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.114, F1 Score: 0.06988058381247236, ROC AUC: 0.8011632750856418\n",
      "\n",
      "SMOTE on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.9470000000000001, F1 Score: 0.31762652705061084, ROC AUC: 0.6818189791785794\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.115, F1 Score: 0.07159142726858185, ROC AUC: 0.7996836228270289\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.998, F1 Score: 0.3019431988041854, ROC AUC: 0.7015627029169681\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.107, F1 Score: 0.08415217939027463, ROC AUC: 0.8214351900710419\n"
     ]
    }
   ],
   "source": [
    "from datastore import DataStore\n",
    "\n",
    "Data = DataStore()\n",
    "random = SampledDataStore.getRandomSampled\n",
    "smote = SampledDataStore.getSMOTESampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [random, smote, smote_enn]\n",
    "sampler_names = [\"Random Oversampling\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}    \n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    probs = trainAndgetProbabilities(X_resampled, y_resampled, Data.getxTest())\n",
    "    f1_scores = [f1_score(Data.getyTest(), convert_probs(probs, t)) for t in thresholds]\n",
    "    roc_scores = [roc_auc_score(Data.getyTest(), convert_probs(probs, t)) for t in thresholds]\n",
    "    maximize_f1 = np.argmax(f1_scores)\n",
    "    maximize_roc = np.argmax(roc_scores)\n",
    "    print(f\"\\n{sampler_names[i]} on test set:\")\n",
    "    print(\"Maxiziming F1 Score:\")\n",
    "    print(f\"Threshold: {thresholds[maximize_f1]}, F1 Score: {f1_scores[maximize_f1]}, ROC AUC: {roc_scores[maximize_f1]}\")\n",
    "    print(\"Maxiziming ROC-AUC Score:\")\n",
    "    print(f\"Threshold: {thresholds[maximize_roc]}, F1 Score: {f1_scores[maximize_roc]}, ROC AUC: {roc_scores[maximize_roc]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but not ideal. The difference in ROC_AUC score points to the problem; The higher threshold value causes the model to predict smaller number of samples to be positive (true positive or false positive), resulting in lower ROC AUC and a higher F1 score.\n",
    "\n",
    "Overall, our results are better than the baseline model, but not ideal. Perhaps, we can achieve better results with a more complex (non-linear) model. Let's try SVM next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
