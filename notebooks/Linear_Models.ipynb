{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/processed/train_data_baseline.csv\")\n",
    "test_data = pd.read_csv(\"../data/processed/test_data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_add_attributes(BaseEstimator, TransformerMixin):\n",
    "    '''Custom transformer based on Sklearn's classes.\n",
    "    Takes in dataframe (train or test) and adds new features and returns\n",
    "    a filtered version of the original train/test datasets.'''\n",
    "    def fit(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        '''Calculates and adds comment body length and account activity (based on frequency of comment author)\n",
    "        as features. Returns a new dataframe with the added columns.\n",
    "        \n",
    "        Arguments:\n",
    "            X: dataframe to transform (train or test)\n",
    "        Returns:\n",
    "            Transformed and filtered version of train or test set\n",
    "        '''\n",
    "        data = X.copy()\n",
    "        data[\"body_len\"] = data.comment_body.apply(lambda x: len(x))\n",
    "        data[\"acc_activity\"] = data.author_ids.map(data.author_ids.value_counts())\n",
    "        data[\"is_premium\"] = data.is_premium.astype(int)\n",
    "        result = data.filter(items=[\"ups\", \"comment_karma\", \"link_karma\", \"is_premium\", \n",
    "                                  \"comment_age_days\", \"acc_age_days\", \"body_len\", \"acc_activity\"], axis=1)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('filter_add', filter_add_attributes()),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_data)\n",
    "X_test = pipeline.transform(test_data)\n",
    "y_train = train_data[\"gildings\"].to_list()\n",
    "y_test = test_data[\"gildings\"].to_list()\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will defined appropriate functions for training and returning cross validation results for our logistic regression model.\n",
    "\n",
    "Due to the high class imbalance, F1 score is a much better metric to use than just accuracy (since 99% of the data belongs to class 0). We will also have ROC-AUC for comparison.\n",
    "\n",
    "We fetch the true positives, false positives and false negatives to calculate the f1 score across all folds rather than using the builtin functionality. This is because the averaged f1 score returned by Sklearn is slighly biased for imbalanced class problems (for cross validation). This doesn't matter when evaluating the test set.\n",
    "\n",
    "Note that we do include the default implementation of f1 score metric in scoring; this is for later use (to tune GridSearch).\n",
    "\n",
    "Reference: https://www.hpl.hp.com/techreports/2009/HPL-2009-359.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the functions to get true positives...false negatives for Sklearn's make_scorer function\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp': make_scorer(tp), 'fp': make_scorer(fp), \n",
    "           'fn': make_scorer(fn), 'roc_auc': make_scorer(roc_auc_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "def train(clf, X, y):\n",
    "    '''Takes in train and train datasets along a model to train and evaluate. Returns f1 and roc-auc scores.\n",
    "    \n",
    "    Arguments:\n",
    "        clf: classifier to use\n",
    "        X: Dataframe corresponding to training set\n",
    "        y: Targets corresponding to the training set\n",
    "    Returns:\n",
    "        Cross_validate output.\n",
    "    '''\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    scores = cross_validate(clf, X, y, cv=rskf, scoring=scoring)\n",
    "    return scores\n",
    "\n",
    "def calc_metrics(scores):\n",
    "    '''Takes in cross_validate outputs, gets arrays of true positives, false negatives and false positives \n",
    "    (corresponding to class 1; gilded) across k folds and calculates the f1 score. Calculates the mean of\n",
    "    roc_auc scores from k folds. Returns both f1 and roc_auc scores.\n",
    "    \n",
    "    Arguments:\n",
    "        Cross_validate outputs\n",
    "    Returns:\n",
    "        F1 score, calculated using the totals, as well as ROC_AUC score from scores.\n",
    "        ROC_AUC score, for the best parameters\n",
    "    '''\n",
    "    tps, fns, fps, roc = scores['test_tp'], scores['test_fn'], scores['test_fp'], scores['test_roc_auc']\n",
    "    tp = np.sum(tps)\n",
    "    fn = np.sum(fns)\n",
    "    fp = np.sum(fps)\n",
    "    f1 = (2*tp)/(2*tp+fn+fp)\n",
    "    return f1, np.mean(scores['test_roc_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish a baseline model that simply predicts the minority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.0038792557287250116\n",
      "ROC-AUC is: 0.5\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "scores = train(clf, X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model is one that can perform better than the baseline, in terms of F1 Score. Anything below is worse than a model that simply predicts minority class.\n",
    "\n",
    "Note that 0.5 ROC-AUC score indicates that it's a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.14389101745423585\n",
      "ROC-AUC is: 0.5423767409646068\n"
     ]
    }
   ],
   "source": [
    "scores = train(LogisticRegression(), X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's slightly better than a random classifier; this means that our model is learning some relationships for the underlying data, albeit small.\n",
    "\n",
    "The low score is to expected, especially given the class imbalance. Let's try using the class weight functionality that assigns weights to each class based on their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.07312321528648821\n",
      "ROC-AUC is: 0.8275665348137995\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight='balanced')\n",
    "scores = train(clf, X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.31030202, 1.16940689, 1.14577937, 1.09115148, 1.13671255,\n",
      "       1.17197299, 1.08603263, 1.18000984, 1.13265562, 1.13404036,\n",
      "       1.18123364, 1.14052343, 1.13511944, 1.04346681, 1.08995152,\n",
      "       1.24200678, 1.08979511, 1.1988821 , 1.03887463, 1.15954828]), 'score_time': array([0.19030547, 0.18034387, 0.18628407, 0.18020415, 0.18183947,\n",
      "       0.18124151, 0.17997718, 0.18089461, 0.18050003, 0.18194246,\n",
      "       0.18317223, 0.18202949, 0.18137407, 0.18184066, 0.1816113 ,\n",
      "       0.18208528, 0.18212032, 0.17977977, 0.18108582, 0.18056846]), 'test_tp': array([71, 72, 71, 66, 63, 70, 64, 72, 75, 61, 65, 79, 69, 76, 63, 66, 72,\n",
      "       69, 65, 61]), 'test_fp': array([1771, 1869, 1786, 1420, 1483, 1729, 1724, 1771, 2053, 1462, 1779,\n",
      "       1964, 1688, 2005, 1684, 1628, 1636, 1844, 1367, 1448]), 'test_fn': array([28, 27, 28, 33, 36, 30, 36, 28, 25, 39, 34, 20, 30, 23, 36, 34, 28,\n",
      "       31, 35, 39]), 'test_roc_auc': array([0.84125709, 0.84534869, 0.84111032, 0.81943901, 0.80367105,\n",
      "       0.83308186, 0.80313079, 0.84267089, 0.85491154, 0.79069444,\n",
      "       0.81087578, 0.87977268, 0.83196821, 0.86421999, 0.80170432,\n",
      "       0.81407014, 0.84399186, 0.82695659, 0.811624  , 0.79083143]), 'test_f1': array([0.07315817, 0.07058824, 0.07259714, 0.08328076, 0.07659574,\n",
      "       0.07372301, 0.06779661, 0.0741122 , 0.06732496, 0.07516944,\n",
      "       0.06690685, 0.07376284, 0.07435345, 0.06972477, 0.06825569,\n",
      "       0.0735786 , 0.07964602, 0.0685544 , 0.0848564 , 0.07582349])}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.07312321528648821\n",
      "ROC-AUC is: 0.8275665348137995\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(class_weight='balanced')\n",
    "scores = train(clf, X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the balanced class weight performs worse in terms of f1 score (probably because it results in a lot more false positives).\n",
    "\n",
    "Let's define functions to test different parameters using GridSearchCV. Note that we are also defining a custom function to calculate the f1 score (as earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grid(X_train, y_train, parameters):\n",
    "    '''Function to train and find the best hyperparameters for Logistic Regression.\n",
    "    Uses GridSearch and crossvalidation for tuning.\n",
    "    \n",
    "    Arguments:\n",
    "        X_train: Training set (pandas Dataframe)\n",
    "        y_train: Targets corresponding to the training set\n",
    "        parameters: Parameters to test/run GridSearch with.\n",
    "        \n",
    "    Returns:\n",
    "        Classifier, which is used to parse the outputs/metrics.\n",
    "    '''\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    clf = GridSearchCV(model, parameters, scoring=scoring, cv=rskf, refit='f1')\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def calc_metrics_grid(clf):\n",
    "    '''Arguments:\n",
    "        Classifier which is used to parse all the relevant outputs\n",
    "    Returns: \n",
    "        F1 score, calculated with True Positives, False Positives and False Negatives across k (k=20) folds\n",
    "    '''\n",
    "    ind = clf.best_index_\n",
    "    tp_list = ['split' + str(i) + '_test_tp' for i in range(20)]\n",
    "    fp_list = ['split' + str(i) + '_test_fp' for i in range(20)]\n",
    "    fn_list = ['split' + str(i) + '_test_fn' for i in range(20)]\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for tp in tp_list:\n",
    "        tps+= clf.cv_results_[tp][ind]\n",
    "    for fp in fp_list:\n",
    "        fps+= clf.cv_results_[fp][ind]\n",
    "    for fn in fn_list:\n",
    "        fns+= clf.cv_results_[fn][ind]\n",
    "    f1 = (2*tps)/(2*fps+fns+fps)\n",
    "    return f1, clf.cv_results_['mean_test_roc_auc'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.2146809295462929\n",
      "ROC-AUC is: 0.6456231724605754\n",
      "Best class weights: {'class_weight': {0: 1, 1: 10}}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:1,1:1000}, {0:10,1:1}]}\n",
    "clf = train_grid(X_train, y_train, parameters)\n",
    "best = clf.best_params_\n",
    "f1, roc = calc_metrics_grid(clf)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best class weights: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making progress, but can we do even better?\n",
    "\n",
    "Adjusting the weights were not enough, we will have to try different sampling techniques. Imbalanced-learn library will come in handy here.\n",
    "\n",
    "We will start with RandomOverSampler to duplicates records from the minority class. We will use a sampling ratio of 0.1 (i.e. ~10% increase in gilded class).\n",
    "\n",
    "Read more: https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#a-practical-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampling:\n",
      "F1 score is: 0.6781609747429389\n",
      "ROC-AUC is: 0.828094671658085\n"
     ]
    }
   ],
   "source": [
    "#Using RandomOverSampler to duplicate records belonging to class 1 (gilded)\n",
    "\n",
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = random_sampler.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "scores = train(clf, X_resampled, y_resampled)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(\"Random Over Sampling:\")\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate new samples with SMOTE and ADASYN based on existing samples. We will keep the sampling ratio the same for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE :\n",
      "F1 score is: 0.6098306864417903\n",
      "ROC-AUC is: 0.7254990149796094\n",
      "\n",
      "\n",
      "ADASYN :\n",
      "F1 score is: 0.5570636250902797\n",
      "ROC-AUC is: 0.6984619951518117\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using SMOTE and ADASYN to generate samples in gilded class\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "ada = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "models = [smote, ada]\n",
    "model_names = [\"SMOTE\", \"ADASYN\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    clf = LogisticRegression(class_weight=[{0: 1, 1: 10}])\n",
    "    scores = train(clf, X_resampled, y_resampled)\n",
    "    f1, roc = calc_metrics(scores)\n",
    "    print(f\"{model_names[i]} :\")\n",
    "    print(f\"F1 score is: {f1}\")\n",
    "    print(f\"ROC-AUC is: {roc}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learn also recommends combining oversampling with undersampling the majority class.\n",
    "\n",
    "Ref: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html\n",
    "\n",
    "SMOTE can generate noisy samples (ex: when classes cannot be well separated), undersampling allows to clean the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE TOMEK :\n",
      "F1 score is: 0.6117789251041132\n",
      "ROC-AUC is: 0.7265310228680512\n",
      "\n",
      "\n",
      "SMOTE ENN :\n",
      "F1 score is: 0.7003509844764216\n",
      "ROC-AUC is: 0.7759026910883595\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_tomek = SMOTETomek(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "\n",
    "models = [smote_tomek, smote_enn]\n",
    "model_names = [\"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    clf = LogisticRegression(class_weight=[{0: 1, 1: 10}])\n",
    "    scores = train(clf, X_resampled, y_resampled)\n",
    "    f1, roc = calc_metrics(scores)\n",
    "    print(f\"{model_names[i]} :\")\n",
    "    print(f\"F1 score is: {f1}\")\n",
    "    print(f\"ROC-AUC is: {roc}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE, SMOTEENN and RandomOverSampler produces the best results so far. Let's evaluate those them on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Oversampling on test set:\n",
      "F1 score: 0.0638904734740445\n",
      "ROC_AUC score: 0.8183981729232407\n",
      "Balanced accuracy score: 0.8183981729232408\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.060296191819464044\n",
      "ROC_AUC score: 0.8228175600742684\n",
      "Balanced accuracy score: 0.8228175600742684\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.08897775556110973\n",
      "ROC_AUC score: 0.8434413510604898\n",
      "Balanced accuracy score: 0.8434413510604898\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "models = [random_sampler, smote, smote_enn]\n",
    "model_names = [\"Random Oversampling\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    grid_res = train_grid(X_resampled, y_resampled, parameters)\n",
    "    y_preds = grid_res.predict(X_test)\n",
    "    print(f\"{model_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(y_test, y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(y_test, y_preds)}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score(y_test, y_preds)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression predicts the class probabilities for each sample and decides class based on a threshold (default: 0.5). We can also check if a different threshold value produces better results.\n",
    "\n",
    "Ref: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "\n",
    "Let's define the relevant functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_probs(X, y, X_test):\n",
    "    '''Takes in train and train datasets along a model to train and evaluate. Returns f1 and roc-auc scores.\n",
    "    Arguments:\n",
    "        X: Training dataset\n",
    "        y: Targets corresponding to the training set\n",
    "        X_test: Test dataset for calculating class probabilities\n",
    "    Returns:\n",
    "        Class probabilities associated with each sample (only returns probabilities for class 1)'''\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    model = LogisticRegressionCV(cv=rskf, class_weight=[{0: 1, 1: 10}])\n",
    "    model.fit(X, y)\n",
    "    return model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def convert_probs(probs, threshold):\n",
    "    return (probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Oversampling on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.9470000000000001, F1 Score: 0.3143350604490501, ROC AUC: 0.681795495628806\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.114, F1 Score: 0.06988058381247236, ROC AUC: 0.8011632750856418\n",
      "\n",
      "SMOTE on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.9470000000000001, F1 Score: 0.31762652705061084, ROC AUC: 0.6818189791785794\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.115, F1 Score: 0.07159142726858185, ROC AUC: 0.7996836228270289\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "Maxiziming F1 Score:\n",
      "Threshold: 0.998, F1 Score: 0.3019431988041854, ROC AUC: 0.7015627029169681\n",
      "Maxiziming ROC-AUC Score:\n",
      "Threshold: 0.107, F1 Score: 0.08415217939027463, ROC AUC: 0.8214351900710419\n"
     ]
    }
   ],
   "source": [
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "models = [random_sampler, smote, smote_enn]\n",
    "model_names = [\"Random Oversampling\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    probs = train_eval_probs(X_resampled, y_resampled, X_test)\n",
    "    f1_scores = [f1_score(y_test, convert_probs(probs, t)) for t in thresholds]\n",
    "    auc_scores = [roc_auc_score(y_test, convert_probs(probs, t)) for t in thresholds]\n",
    "    ind1 = np.argmax(f1_scores)\n",
    "    ind2 = np.argmax(auc_scores)\n",
    "    print(f\"\\n{model_names[i]} on test set:\")\n",
    "    print(\"Maxiziming F1 Score:\")\n",
    "    print(f\"Threshold: {thresholds[ind1]}, F1 Score: {f1_scores[ind1]}, ROC AUC: {auc_scores[ind1]}\")\n",
    "    print(\"Maxiziming ROC-AUC Score:\")\n",
    "    print(f\"Threshold: {thresholds[ind2]}, F1 Score: {f1_scores[ind2]}, ROC AUC: {auc_scores[ind2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our results are better than our baseline model, but not ideal. Perhaps, we can achieve better results with a more complex (non-linear) model. Let's try SVM next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
