{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/processed/train_data_baseline.csv\")\n",
    "test_data = pd.read_csv(\"../data/processed/test_data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_add_attributes(BaseEstimator, TransformerMixin):\n",
    "    '''Custom transformer based on Sklearn's classes.\n",
    "    Takes in dataframe (train or test) and adds new features and returns\n",
    "    a filtered version of the original train/test datasets.'''\n",
    "    def fit(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        '''Calculates and adds comment body length and account activity (based on frequency of comment author)\n",
    "        as features. Returns a new dataframe with the added columns.'''\n",
    "        data = X.copy()\n",
    "        data[\"body_len\"] = data.comment_body.apply(lambda x: len(x))\n",
    "        data[\"acc_activity\"] = data.author_ids.map(data.author_ids.value_counts())\n",
    "        data[\"is_premium\"] = data.is_premium.astype(int)\n",
    "        return data.filter(items=[\"ups\", \"comment_karma\", \"link_karma\", \"is_premium\", \"comment_age_days\", \"acc_age_days\", \"body_len\", \"acc_activity\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('filter_add', filter_add_attributes()),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_data)\n",
    "X_test = pipeline.transform(test_data)\n",
    "y_train = train_data[\"gildings\"].to_list()\n",
    "y_test = test_data[\"gildings\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the functions to get true positives...false negatives for Sklearn's make_scorer function\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp': make_scorer(tp), 'fp': make_scorer(fp), \n",
    "           'fn': make_scorer(fn), 'roc_auc': make_scorer(roc_auc_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "def train(clf, X, y):\n",
    "    '''Takes in train and train datasets along a model to train and evaluate. Returns f1 and roc-auc scores.\n",
    "    \n",
    "    Arguments:\n",
    "        clf: classifier to use\n",
    "        X: Dataframe corresponding to training set\n",
    "        y: Targets corresponding to the training set\n",
    "    Returns:\n",
    "        Cross_validate output.\n",
    "    '''\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    scores = cross_validate(clf, X, y, cv=rskf, scoring=scoring, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "def calc_metrics(scores):\n",
    "    '''Takes in cross_validate outputs, gets arrays of true positives, false negatives and false positives \n",
    "    (corresponding to class 1; gilded) across k folds and calculates the f1 score. Calculates the mean of\n",
    "    roc_auc scores from k folds. Returns both f1 and roc_auc scores.\n",
    "    \n",
    "    Arguments:\n",
    "        Cross_validate outputs\n",
    "    Returns:\n",
    "        F1 score, calculated using the totals, as well as ROC_AUC score from scores.\n",
    "    '''\n",
    "    tps, fns, fps, roc = scores['test_tp'], scores['test_fn'], scores['test_fp'], scores['test_roc_auc']\n",
    "    tp = np.sum(tps)\n",
    "    fn = np.sum(fns)\n",
    "    fp = np.sum(fps)\n",
    "    f1 = (2*tp)/(2*tp+fn+fp)\n",
    "    return f1, np.mean(scores['test_roc_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular SVM doesn't scale well to our particular problem (due to the size of the dataset and limited resources available). So, we will use Stochastic Gradient Descent with hinge loss (equates to SVM) instead. Let's try with the default parameters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.07898320472083523\n",
      "ROC-AUC is: 0.5217944162821552\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",random_state=42)\n",
    "scores = train(clf, X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the baseline. Let's try assigning weights to class. As with logistic regression, we will start off with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.018119683335791906\n",
      "ROC-AUC is: 0.7924024522725649\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",random_state=42, class_weight='balanced')\n",
    "scores = train(clf, X_train, y_train)\n",
    "f1, roc = calc_metrics(scores)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the functions from Logistic Regression notebook to test SGD on various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grid(X_train, y_train, parameters):\n",
    "    '''Function to train and find the best hyperparameters for SVM (SGD).\n",
    "    Uses GridSearch and crossvalidation for tuning.\n",
    "    \n",
    "    Arguments:\n",
    "        X_train: Training set (pandas Dataframe)\n",
    "        y_train: Targets corresponding to the training set\n",
    "        parameters: Parameters to test/run GridSearch with.\n",
    "        \n",
    "    Returns:\n",
    "        Classifier, which is used to parse the outputs/metrics.\n",
    "    '''\n",
    "    model = SGDClassifier(loss=\"hinge\", random_state=42, max_iter=2000)\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    clf = GridSearchCV(model, parameters, scoring=scoring, cv=rskf, refit='f1', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def calc_metrics_grid(clf):\n",
    "    '''Arguments:\n",
    "        Classifier which is used to parse all the relevant outputs\n",
    "    Returns: \n",
    "        F1 score, calculated with True Positives, False Positives and False Negatives across k (k=20) folds\n",
    "        ROC_AUC score, for the best parameters\n",
    "    '''\n",
    "    ind = clf.best_index_\n",
    "    tp_list = ['split' + str(i) + '_test_tp' for i in range(20)]\n",
    "    fp_list = ['split' + str(i) + '_test_fp' for i in range(20)]\n",
    "    fn_list = ['split' + str(i) + '_test_fn' for i in range(20)]\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for tp in tp_list:\n",
    "        tps+= clf.cv_results_[tp][ind]\n",
    "    for fp in fp_list:\n",
    "        fps+= clf.cv_results_[fp][ind]\n",
    "    for fn in fn_list:\n",
    "        fns+= clf.cv_results_[fn][ind]\n",
    "    f1 = (2*tps)/(2*fps+fns+fps)\n",
    "    return f1, clf.cv_results_['mean_test_roc_auc'][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.14700104010169884\n",
      "ROC-AUC is: 0.6586455337266552\n",
      "Best class weights: {'class_weight': {0: 1, 1: 10}}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:10,1:1}]}\n",
    "clf = train_grid(X_train, y_train, parameters)\n",
    "best = clf.best_params_\n",
    "f1, roc = calc_metrics_grid(clf)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best class weights: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse F1 score (0.14 vs 0.21) compared to our results from weighted Logistic regression (same best weights) but comparable ROC-AUC score (0.65 vs 0.64).\n",
    "\n",
    "SGD uses 2000 iterations (due to no convergence in the next block; just to check if we can get better results).\n",
    "\n",
    "This may suggest that SVM is not a good model for our problem. However before we make any conclusions, let's \n",
    "fit the model with several other values for hyperparameters (such as using l1 norm or elastic net). We will also experiment with different values for alpha (regularization strength).\n",
    "\n",
    "Note: l1_ratio is only used for elasticnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.20868322315526175\n",
      "ROC-AUC is: 0.6461119840565462\n",
      "Best class weights: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'penalty':['l2', 'l1', 'elasticnet'], 'l1_ratio': [0.15, 0.30], \n",
    "              'alpha' : [0.0001, 0.001, 0.01, 0.1] , 'class_weight':[{0:1,1:10}]}\n",
    "clf = train_grid(X_train, y_train, parameters)\n",
    "best = clf.best_params_\n",
    "f1, roc = calc_metrics_grid(clf)\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best class weights: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these hyperparameter values for sampling methods; we only need to set the class weight and alpha as SGD uses l2 norm (penalty) as default. For comparison, we will also run the models with default alpha value (0.001).\n",
    "\n",
    "We will start with RandomOverSampler to duplicates records from the minority class. We will use a sampling ratio of 0.1 (i.e. ~10% increase in gilded class).\n",
    "\n",
    "As with Logistic Regression, we will try experimenting with oversampling techniques. Let's go try RandomOverSampler, SMOTE and ADASYN first.\n",
    "\n",
    "RandomOverSampler duplicates samples belonging to minority class (gilded) while SMOTE and ADASYN creates synthentic samples that are similar to true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OverSampler. 0.001:\n",
      "F1 score is: 0.7301899218218899\n",
      "ROC-AUC is: 0.8278357489175485\n",
      "\n",
      "\n",
      "Random OverSampler. 0.01:\n",
      "F1 score is: 0.7290762216545105\n",
      "ROC-AUC is: 0.8203429769984393\n",
      "\n",
      "\n",
      "SMOTE. 0.001:\n",
      "F1 score is: 0.7237177851492728\n",
      "ROC-AUC is: 0.8242999468971522\n",
      "\n",
      "\n",
      "SMOTE. 0.01:\n",
      "F1 score is: 0.7205503573713654\n",
      "ROC-AUC is: 0.8150874355518173\n",
      "\n",
      "\n",
      "ADASYN. 0.001:\n",
      "F1 score is: 0.696385671963372\n",
      "ROC-AUC is: 0.8075312297638717\n",
      "\n",
      "\n",
      "ADASYN. 0.01:\n",
      "F1 score is: 0.6887483633729838\n",
      "ROC-AUC is: 0.7952816823356728\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "ada = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "models = [random_sampler, smote, ada]\n",
    "model_names = [\"Random OverSampler\", \"SMOTE\", \"ADASYN\"]\n",
    "alpha_vals = [0.001, 0.01]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    for j in range(len(alpha_vals)):\n",
    "        clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=alpha_vals[j], random_state=42)\n",
    "        scores = train(clf, X_resampled, y_resampled)\n",
    "        f1, roc = calc_metrics(scores)\n",
    "        print(f\"{model_names[i]}. {alpha_vals[j]}:\")\n",
    "        print(f\"F1 score is: {f1}\")\n",
    "        print(f\"ROC-AUC is: {roc}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as big of a difference between 0.001 and 0.01 as reg strength.\n",
    "\n",
    "SMOTE can generate noisy samples (ex: when classes cannot be well separated). In such cases, Imbalanced learn recommends combining oversampling with undersampling the majority class. This can be done through SMOTETomek and SMOTEENN.\n",
    "\n",
    "Ref: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE TOMEK. 0.001:\n",
      "F1 score is: 0.72427746895832\n",
      "ROC-AUC is: 0.824775953000174\n",
      "\n",
      "\n",
      "SMOTE TOMEK. 0.01:\n",
      "F1 score is: 0.7212362485420306\n",
      "ROC-AUC is: 0.8156752106861175\n",
      "\n",
      "\n",
      "SMOTE ENN. 0.001:\n",
      "F1 score is: 0.7594824714797236\n",
      "ROC-AUC is: 0.8581641569754759\n",
      "\n",
      "\n",
      "SMOTE ENN. 0.01:\n",
      "F1 score is: 0.7514495425143278\n",
      "ROC-AUC is: 0.8330756751889987\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_tomek = SMOTETomek(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "models = [smote_tomek, smote_enn]\n",
    "model_names = [\"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "alpha_vals = [0.001, 0.01]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    for j in range(len(alpha_vals)):\n",
    "        clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=alpha_vals[j], random_state=42)\n",
    "        scores = train(clf, X_resampled, y_resampled)\n",
    "        f1, roc = calc_metrics(scores)\n",
    "        print(f\"{model_names[i]}. {alpha_vals[j]}:\")\n",
    "        print(f\"F1 score is: {f1}\")\n",
    "        print(f\"ROC-AUC is: {roc}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Logistic regression, SMOTEENN and RandomOverSampler produces the best results (though the results are much closer with SGD; note that SMOTE results are very close as well). Let's try evaluating on our test set (for both f1 score and accuracy) with 0.001 as the regularization strength (default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Oversampling on test set:\n",
      "F1 score: 0.12220566318926976\n",
      "ROC_AUC score: 0.8204287454870794\n",
      "Balanced accuracy score: 0.8204287454870794\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.1163039600428113\n",
      "ROC_AUC score: 0.8179510423630978\n",
      "Balanced accuracy score: 0.8179510423630978\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.08817733990147783\n",
      "ROC_AUC score: 0.8452223755411945\n",
      "Balanced accuracy score: 0.8452223755411944\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "models = [random_sampler, smote, smote_enn]\n",
    "model_names = [\"Random Oversampling\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    grid_res = train_grid(X_resampled, y_resampled, parameters)\n",
    "    y_preds = grid_res.predict(X_test)\n",
    "    print(f\"{model_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(y_test, y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(y_test, y_preds)}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score(y_test, y_preds)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three methods have achieved better F1 scores and comparable ROC_AUC/balanced accuracy scores than Logistic Regression (0.06). We will try Decision Trees next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
