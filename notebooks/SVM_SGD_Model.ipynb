{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/processed/train_data_baseline.csv\")\n",
    "test_data = pd.read_csv(\"../data/processed/test_data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_add_attributes(BaseEstimator, TransformerMixin):\n",
    "    '''Custom transformer based on Sklearn's classes.\n",
    "    Takes in dataframe (train or test) and adds new features and returns\n",
    "    a filtered version of the original train/test datasets.'''\n",
    "    def fit(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        '''Calculates and adds comment body length and account activity (based on frequency of comment author)\n",
    "        as features. Returns a new dataframe with the added columns.'''\n",
    "        data = X.copy()\n",
    "        data[\"body_len\"] = data.comment_body.apply(lambda x: len(x))\n",
    "        data[\"acc_activity\"] = data.author_ids.map(data.author_ids.value_counts())\n",
    "        data[\"is_premium\"] = data.is_premium.astype(int)\n",
    "        return data.filter(items=[\"ups\", \"comment_karma\", \"link_karma\", \"is_premium\", \"comment_age_days\", \"acc_age_days\", \"body_len\", \"acc_activity\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('filter_add', filter_add_attributes()),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_data)\n",
    "X_test = pipeline.transform(test_data)\n",
    "y_train = train_data[\"gildings\"].to_list()\n",
    "y_test = test_data[\"gildings\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(clf, X, y):\n",
    "    '''Takes in train and train datasets along a model to train and evaluate. Returns f1 and roc-auc scores.'''\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    scores = cross_validate(clf, X, y, cv=rskf, scoring=['f1', 'roc_auc'], n_jobs=-1)\n",
    "    return scores['test_f1'], scores['test_roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular SVM doesn't scale well to our particular problem (due to the size of the dataset and limited resources available). So, we will use Stochastic Gradient Descent with hinge loss (equates to SVM) instead. Let's try with the default parameters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.07231340451914046\n",
      "ROC-AUC is: 0.6709049027449308\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",random_state=42)\n",
    "f1, roc = train_eval(clf, X_train, y_train)\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the baseline. Let's try assigning weights to class. As with logistic regression, we will start off with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.028938602553323745\n",
      "ROC-AUC is: 0.8750954918662222\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\",random_state=42, class_weight='balanced')\n",
    "f1, roc = train_eval(clf, X_train, y_train)\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_weights(X_train, y_train, parameters):\n",
    "    '''Function to train and evalute SGD Classifier (w/ Cross Validation) Model.'''\n",
    "    model = SGDClassifier(loss=\"hinge\")\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    grid = GridSearchCV(model, parameters, scoring=['f1', 'roc_auc'], cv=rskf, refit='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: 0.23172721564368265 and Best parameters: {'class_weight': {0: 1, 1: 10}}\n",
      "\n",
      "F1 Score: 0.0833, ROC_AUC: 0.6834, parameters: {'class_weight': {0: 1, 1: 1}}\n",
      "F1 Score: 0.2317, ROC_AUC: 0.8574, parameters: {'class_weight': {0: 1, 1: 10}}\n",
      "F1 Score: 0.1319, ROC_AUC: 0.8911, parameters: {'class_weight': {0: 1, 1: 100}}\n",
      "F1 Score: 0.013, ROC_AUC: 0.8804, parameters: {'class_weight': {0: 1, 1: 1000}}\n",
      "F1 Score: 0.0047, ROC_AUC: 0.8603, parameters: {'class_weight': {0: 1, 1: 10000}}\n",
      "F1 Score: 0.0271, ROC_AUC: 0.8565, parameters: {'class_weight': {0: 10, 1: 1}}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:1,1:1000}, {0:1,1:10000}, {0:10,1:1}]}\n",
    "grid_res = train_eval_weights(X_train, y_train, parameters)\n",
    "print(f\"Best results: {grid_res.best_score_} and Best parameters: {grid_res.best_params_}\\n\")\n",
    "f1s = grid_res.cv_results_['mean_test_f1']\n",
    "rocs = grid_res.cv_results_['mean_test_roc_auc']\n",
    "params = grid_resgrid_res.cv_results_['params']\n",
    "for f1, roc, param in zip(f1s, rocs, params):\n",
    "    print(f\"F1 Score: {round(f1,4)}, ROC_AUC: {round(roc,4)}, parameters: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, better than our result from weighted Logistic regression (F1 score: 0.12); comparable ROC-AUC score as well (ROC-AUC: 0.88).\n",
    "\n",
    "We can also fit with model with several other values for hyperparameters (such as using l1 norm or elastic net). We will also experiment with different values for alpha (regularization strength).\n",
    "\n",
    "Note: l1_ratio is only used for elasticnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results: 0.2954858955870661 and Best parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2436, ROC_AUC: 0.8557, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2417, ROC_AUC: 0.8657, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2598, ROC_AUC: 0.8838, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2489, ROC_AUC: 0.8557, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2324, ROC_AUC: 0.8689, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2465, ROC_AUC: 0.8722, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2405, ROC_AUC: 0.8719, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2505, ROC_AUC: 0.8628, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.261, ROC_AUC: 0.877, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2469, ROC_AUC: 0.8602, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2526, ROC_AUC: 0.8727, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2732, ROC_AUC: 0.9005, parameters: {'alpha': 0.0001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2614, ROC_AUC: 0.8883, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2742, ROC_AUC: 0.8462, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2798, ROC_AUC: 0.898, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2769, ROC_AUC: 0.8839, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2457, ROC_AUC: 0.845, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2652, ROC_AUC: 0.8922, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2751, ROC_AUC: 0.8838, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2701, ROC_AUC: 0.8585, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2905, ROC_AUC: 0.8796, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2693, ROC_AUC: 0.8924, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2692, ROC_AUC: 0.8432, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2742, ROC_AUC: 0.8777, parameters: {'alpha': 0.001, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2899, ROC_AUC: 0.8889, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.259, ROC_AUC: 0.9416, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2901, ROC_AUC: 0.891, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2887, ROC_AUC: 0.8782, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2586, ROC_AUC: 0.919, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.286, ROC_AUC: 0.9035, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.3, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2826, ROC_AUC: 0.8946, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2095, ROC_AUC: 0.9202, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2762, ROC_AUC: 0.9272, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.6, 'penalty': 'elasticnet'}\n",
      "\n",
      "F1 Score: 0.2955, ROC_AUC: 0.8793, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l2'}\n",
      "\n",
      "F1 Score: 0.2444, ROC_AUC: 0.917, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'l1'}\n",
      "\n",
      "F1 Score: 0.2724, ROC_AUC: 0.9346, parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.9, 'penalty': 'elasticnet'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'penalty':['l2', 'l1', 'elasticnet'], 'l1_ratio': [0.15, 0.30, 0.60, 0.90], \n",
    "              'alpha' : [0.0001, 0.001, 0.01] , 'class_weight':[{0:1,1:10}]}\n",
    "grid_res = train_eval_weights(X_train, y_train, parameters)\n",
    "print(f\"Best results: {grid_res.best_score_} and Best parameters: {grid_res.best_params_}\\n\")\n",
    "f1s = grid_res.cv_results_['mean_test_f1']\n",
    "rocs = grid_res.cv_results_['mean_test_roc_auc']\n",
    "params = grid_res.cv_results_['params']\n",
    "for f1, roc, param in zip(f1s, rocs, params):\n",
    "    print(f\"F1 Score: {round(f1,4)}, ROC_AUC: {round(roc,4)}, parameters: {param}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these hyperparameter values for sampling methods; we only need to set the class weight and alpha as SGD uses l2 norm (penalty) as default. For comparison, we will also run the models with default alpha value (0.001).\n",
    "\n",
    "We will start with RandomOverSampler to duplicates records from the minority class. We will use a sampling ratio of 0.1 (i.e. ~10% increase in gilded class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampling:\n",
      "F1 score is: 0.7292231443582522\n",
      "ROC-AUC is: 0.9390245759403417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using RandomOverSampler to duplicate records belonging to class 1 (gilded)\n",
    "\n",
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = random_sampler.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10})\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"Random Over Sampling:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to a model with larger regularization (higher alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Over Sampling:\n",
      "F1 score is: 0.728681741124321\n",
      "ROC-AUC is: 0.9448275406809963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using RandomOverSampler to duplicate records belonging to class 1 (gilded)\n",
    "\n",
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = random_sampler.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=0.01)\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"Random Over Sampling:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as big of a difference. How about with other oversampling techniques?\n",
    "\n",
    "We can generate new samples with SMOTE and ADASYN based on existing samples. We will keep the sampling ratio the same for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE:\n",
      "F1 score is: 0.7240862483229289\n",
      "ROC-AUC is: 0.9522434730514199\n",
      "\n",
      "ADASYN:\n",
      "F1 score is: 0.6982751104719969\n",
      "ROC-AUC is: 0.9456775552753072\n",
      "\n",
      " Higher Alpha (0.01)\n",
      ":\n",
      "SMOTE:\n",
      "F1 score is: 0.7205337453810177\n",
      "ROC-AUC is: 0.9611876119095077\n",
      "\n",
      "ADASYN:\n",
      "F1 score is: 0.6887987320458137\n",
      "ROC-AUC is: 0.9549116887190431\n"
     ]
    }
   ],
   "source": [
    "#Using SMOTE to generate samples in gilded class\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10})\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")\n",
    "\n",
    "#Using ADASYN to generate samples in gilded class\n",
    "\n",
    "ada = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10})\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"ADASYN:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\")\n",
    "\n",
    "print(\"\\n Higher Alpha (0.01)\\n:\")\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=0.01)\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")\n",
    "\n",
    "#Using ADASYN to generate samples in gilded class\n",
    "\n",
    "ada = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=0.01)\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"ADASYN:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learn also recommends combining oversampling with undersampling the majority class.\n",
    "\n",
    "Ref: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html\n",
    "\n",
    "SMOTE can generate noisy samples (ex: when classes cannot be well separated), undersampling allows to clean the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE - Tomek's Link:\n",
      "F1 score is: 0.7258589025637112\n",
      "ROC-AUC is: 0.9538878971045017\n",
      "\n",
      "SMOTE - Edited nearest neighbours:\n",
      "F1 score is: 0.7588807574452947\n",
      "ROC-AUC is: 0.9638376085750204\n",
      "\n",
      "\n",
      " Higher Alpha (0.01)\n",
      ":\n",
      "SMOTE - Tomek's Link:\n",
      "F1 score is: 0.7211540744014311\n",
      "ROC-AUC is: 0.9614624304063588\n",
      "\n",
      "SMOTE - Edited nearest neighbours:\n",
      "F1 score is: 0.7514398652168174\n",
      "ROC-AUC is: 0.9674555574777809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_tomek = SMOTETomek(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10})\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE - Tomek's Link:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")\n",
    "\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10})\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE - Edited nearest neighbours:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")\n",
    "\n",
    "print(\"\\n Higher Alpha (0.01)\\n:\")\n",
    "\n",
    "smote_tomek = SMOTETomek(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=0.01)\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE - Tomek's Link:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")\n",
    "\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "assert len(X_resampled) == len(y_resampled)\n",
    "clf = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=0.01)\n",
    "f1, roc = train_eval(clf, X_resampled, y_resampled)\n",
    "print(\"SMOTE - Edited nearest neighbours:\")\n",
    "print(f\"F1 score is: {np.mean(f1)}\")\n",
    "print(f\"ROC-AUC is: {np.mean(roc)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Logistic regression, SMOTEENN and RandomOverSampler produces the best results (though the results are much closer with SGD). Let's try evaluating on our test set (for both f1 score and accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Oversampling on test set:\n",
      "F1 score: 0.12749315604223702\n",
      "ROC_AUC score: 0.8189138679038066\n",
      "Accuracy score: 0.9825700401568774\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.11395027624309394\n",
      "ROC_AUC score: 0.8216109394485629\n",
      "Accuracy score: 0.9799528117626838\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.0875768757687577\n",
      "ROC_AUC score: 0.843190859862907\n",
      "Accuracy score: 0.9710229847341365\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sampler = RandomOverSampler(sampling_strategy=0.1, random_state=42)\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "smote_enn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "models = [random_sampler, smote, smote_enn]\n",
    "model_names = [\"Random Oversampling\", \"SMOTE\", \"SMOTE ENN\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = models[i].fit_resample(X_train, y_train)\n",
    "    grid_res = train_eval_weights(X_resampled, y_resampled, parameters)\n",
    "    y_preds = grid_res.predict(X_test)\n",
    "    print(f\"{model_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(y_test, y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(y_test, y_preds)}\")\n",
    "    print(f\"Accuracy score: {accuracy_score(y_test, y_preds)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Random Oversampling and SMOTE has achieved better results (.12 vs 0.06 in Logistic Regression). We will try Random Forests next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Programming-Files",
   "language": "python",
   "name": "programming-files"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
