{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"functions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datastore import DataStore\n",
    "from searchgrid import SearchGrid\n",
    "from crossvalidate import CrossValidate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sampleddatastore import SampledDataStore as sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load object for CrossValidation\n",
    "crossvalidate = CrossValidate()\n",
    "\n",
    "#Load object for GridSearchCV\n",
    "GridSpace = SearchGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular SVM doesn't scale well to our particular problem (due to the size of the dataset and limited resources available). So, we will use Stochastic Gradient Descent with hinge loss (equates to SVM) instead. Let's try with the default parameters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.08413351623228166\n",
      "ROC-AUC is: 0.5238336749995037\n"
     ]
    }
   ],
   "source": [
    "classifier = SGDClassifier(loss=\"hinge\",random_state=42)\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the baseline. Let's try assigning weights to class. As with logistic regression, we will start off with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.02984576721635957\n",
      "ROC-AUC is: 0.8055003393516408\n"
     ]
    }
   ],
   "source": [
    "classifier = SGDClassifier(loss=\"hinge\", random_state=42, class_weight='balanced')\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering our results with Logistic Regression, we can hypothesize that 1:10 non gilded to gilded ratio produces the best result, but let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.24144226380648107\n",
      "ROC-AUC is: 0.636675772913657\n",
      "Best Parameters: {'class_weight': {0: 1, 1: 10}}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:10,1:1}]}\n",
    "classifier = SGDClassifier(loss=\"hinge\", random_state=42, max_iter=2000)\n",
    "GridSpace.setGridParameters(parameters)\n",
    "GridSpace.setClassifier(classifier)\n",
    "GridSpace.run()\n",
    "parameters, scores = GridSpace.getMetrics().getBestResults()\n",
    "f1 = scores[0]\n",
    "roc = scores[1]\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse F1 score (0.24 vs 0.30) compared to our results from weighted Logistic regression (same best weights) but comparable ROC-AUC score (0.64 vs 0.65).\n",
    "\n",
    "SGD uses 2000 iterations (due to no convergence in the next block; just to check if we can get better results).\n",
    "\n",
    "This may suggest that SVM is not a good model for our problem. However before we make any conclusions, let's \n",
    "fit the model with several other values for hyperparameters (such as using l1 norm or elastic net). We will also experiment with different values for alpha (regularization strength).\n",
    "\n",
    "Note: l1_ratio is only used for elasticnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.28295656591313184\n",
      "ROC-AUC is: 0.6441571799070308\n",
      "Best Parameters: {'alpha': 0.01, 'class_weight': {0: 1, 1: 10}, 'l1_ratio': 0.15, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'penalty':['l2', 'l1', 'elasticnet'], 'l1_ratio': [0.15, 0.30], \n",
    "              'alpha' : [0.0001, 0.001, 0.01, 0.1] , 'class_weight':[{0:1,1:10}]}\n",
    "classifier = SGDClassifier(loss=\"hinge\", random_state=42, max_iter=2000)\n",
    "GridSpace.setGridParameters(parameters)\n",
    "GridSpace.setClassifier(classifier)\n",
    "GridSpace.run()\n",
    "parameters, scores = GridSpace.getMetrics().getBestResults()\n",
    "f1 = scores[0]\n",
    "roc = scores[1]\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these hyperparameter values for sampling methods; we only need to set the class weight and alpha as SGD uses l2 norm (penalty) as default. For comparison, we will also run the models with default alpha value (0.001).\n",
    "\n",
    "As with Logistic Regression, we will try experimenting with different oversampling techniques. Let's go try RandomOverSampler, SMOTE and ADASYN first (0.1 sampling rate; augmenting gilded class to be roughly 10% of total).\n",
    "\n",
    "RandomOverSampler duplicates samples belonging to minority class (gilded) while SMOTE and ADASYN creates synthentic samples that are similar to true ones.\n",
    "\n",
    "We can load up up Sampling data using SampledDataStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sampling Data...\n"
     ]
    }
   ],
   "source": [
    "SampledDataStore = sds()\n",
    "SampledDataStore.initializeSamplers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OverSampler, 0.001:\n",
      "F1 score is: 0.7306697040739594\n",
      "ROC-AUC is: 0.8247817029378792\n",
      "\n",
      "\n",
      "Random OverSampler, 0.01:\n",
      "F1 score is: 0.729177897574124\n",
      "ROC-AUC is: 0.8198285905218491\n",
      "\n",
      "\n",
      "SMOTE, 0.001:\n",
      "F1 score is: 0.7290083922261484\n",
      "ROC-AUC is: 0.8243498900813335\n",
      "\n",
      "\n",
      "SMOTE, 0.01:\n",
      "F1 score is: 0.7275133907288482\n",
      "ROC-AUC is: 0.8193328643619701\n",
      "\n",
      "\n",
      "ADASYN, 0.001:\n",
      "F1 score is: 0.7014310832164875\n",
      "ROC-AUC is: 0.809260044221294\n",
      "\n",
      "\n",
      "ADASYN, 0.01:\n",
      "F1 score is: 0.7001766996620404\n",
      "ROC-AUC is: 0.8016497166444345\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random = SampledDataStore.getRandomSampled\n",
    "smote = SampledDataStore.getSMOTESampled\n",
    "ada = SampledDataStore.getADASYNSampled\n",
    "samplers = [random, smote, ada]\n",
    "sampler_names = [\"Random OverSampler\", \"SMOTE\", \"ADASYN\"]\n",
    "alpha_vals = [0.001, 0.01]\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    crossvalidate.getDataStore().setxTrain(X_resampled)\n",
    "    crossvalidate.getDataStore().setyTrain(y_resampled)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    for j in range(len(alpha_vals)):\n",
    "        classifier = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=alpha_vals[j], random_state=42)\n",
    "        crossvalidate.setClassifier(classifier)\n",
    "        crossvalidate.run()\n",
    "        f1, roc = crossvalidate.getMetrics().getScores()\n",
    "        print(f\"{sampler_names[i]}, {alpha_vals[j]}:\")\n",
    "        print(f\"F1 score is: {f1}\")\n",
    "        print(f\"ROC-AUC is: {roc}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "crossvalidate.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as big of a difference between 0.001 and 0.01 as reg strength.\n",
    "\n",
    "SMOTE can generate noisy samples (ex: when classes cannot be well separated). In such cases, Imbalanced learn recommends combining oversampling with undersampling the majority class. This can be done through SMOTETomek and SMOTEENN.\n",
    "\n",
    "Ref: https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE TOMEK, 0.001:\n",
      "F1 score is: 0.733375342251864\n",
      "ROC-AUC is: 0.8282549602693855\n",
      "\n",
      "\n",
      "SMOTE TOMEK, 0.01:\n",
      "F1 score is: 0.7320765885754617\n",
      "ROC-AUC is: 0.8219595320608377\n",
      "\n",
      "\n",
      "SMOTE ENN, 0.001:\n",
      "F1 score is: 0.7721603956898074\n",
      "ROC-AUC is: 0.8609611811303036\n",
      "\n",
      "\n",
      "SMOTE ENN, 0.01:\n",
      "F1 score is: 0.7771496874766541\n",
      "ROC-AUC is: 0.8463373912084468\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_tomek = SampledDataStore.getSMOTETOMEKSampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [smote_tomek, smote_enn]\n",
    "sampler_names = [\"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "alpha_vals = [0.001, 0.01]      \n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    crossvalidate.getDataStore().setxTrain(X_resampled)\n",
    "    crossvalidate.getDataStore().setyTrain(y_resampled)\n",
    "    assert len(X_resampled) == len(y_resampled)\n",
    "    for j in range(len(alpha_vals)):\n",
    "        classifier = SGDClassifier(loss=\"hinge\", class_weight={0: 1, 1: 10}, alpha=alpha_vals[j], random_state=42)\n",
    "        crossvalidate.setClassifier(classifier)\n",
    "        crossvalidate.run()\n",
    "        f1, roc = crossvalidate.getMetrics().getScores()\n",
    "        print(f\"{sampler_names[i]}, {alpha_vals[j]}:\")\n",
    "        print(f\"F1 score is: {f1}\")\n",
    "        print(f\"ROC-AUC is: {roc}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "crossvalidate.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Logistic regression, SMOTEENN and RandomOverSampler produces the best results (though the results are much closer with SGD; note that SMOTE results are very close as well). Let's try evaluating on our test set (for both f1 score and accuracy) with 0.001 as the regularization strength (default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OverSampler on test set:\n",
      "F1 score: 0.12569610182975338\n",
      "ROC_AUC score: 0.8206467075753549\n",
      "Balanced accuracy score: 0.820646707575355\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.1167608286252354\n",
      "ROC_AUC score: 0.8138168993952279\n",
      "Balanced accuracy score: 0.8138168993952279\n",
      "\n",
      "\n",
      "ADASYN on test set:\n",
      "F1 score: 0.13013420089467262\n",
      "ROC_AUC score: 0.8250428816466552\n",
      "Balanced accuracy score: 0.8250428816466553\n",
      "\n",
      "\n",
      "SMOTE TOMEK on test set:\n",
      "F1 score: 0.11666666666666665\n",
      "ROC_AUC score: 0.8259182812713906\n",
      "Balanced accuracy score: 0.8259182812713904\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.09666848716548333\n",
      "ROC_AUC score: 0.8556841837186643\n",
      "Balanced accuracy score: 0.8556841837186642\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random = SampledDataStore.getRandomSampled\n",
    "smote = SampledDataStore.getSMOTESampled\n",
    "ada = SampledDataStore.getADASYNSampled\n",
    "smote_tomek = SampledDataStore.getSMOTETOMEKSampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [random, smote, ada, smote_tomek, smote_enn]\n",
    "sampler_names = [\"Random OverSampler\", \"SMOTE\", \"ADASYN\", \"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "\n",
    "classifier = SGDClassifier(loss=\"hinge\", random_state=42, max_iter=2000)\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    GridSpace.getDataStore().setxTrain(X_resampled)\n",
    "    GridSpace.getDataStore().setyTrain(y_resampled) \n",
    "    GridSpace.setGridParameters(parameters)\n",
    "    GridSpace.setClassifier(classifier)\n",
    "    grid = GridSpace.run()\n",
    "    y_preds = grid.predict(GridSpace.getDataStore().getxTest())\n",
    "    print(f\"{sampler_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "GridSpace.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the updated feature set, SVM achieved slightly worse results than Logistic Regression (Ex: 0.13 vs 0.14 with Random Sampler; default thresholding).\n",
    "\n",
    "Better F1 score was achieved with modified thresholding, however that is neither suitable or a fair comparison (as reflected in the roc_auc value; we can hypothesize that the model simply predicted less samples to be positive/gilded due to larger threshold - resulting in lower number of true and false positives and a result, smaller roc_auc value).\n",
    "\n",
    "We will try Decision Trees next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
