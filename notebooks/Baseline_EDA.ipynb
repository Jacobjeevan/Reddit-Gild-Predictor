{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_data = pd.read_csv(\"../data/raw/author_data.csv\")\n",
    "comment_data = pd.read_csv(\"../data/raw/comment_data.csv\")\n",
    "gilds_data = pd.read_csv(\"../data/raw/gildings_data.csv\")\n",
    "threads_data = pd.read_csv(\"../data/raw/thread_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the duplicates - I had to run a modified version of the script since desktop kept shutting down (due to inclement weather; used append flag as a quick fix which resulted in all of the data being written multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_na(data, key_columns):\n",
    "    '''Takes in data and key columns as parameters, in this case, key column refers to\n",
    "    the columns to consider for identifying duplicates. Returns a dataframe where the \n",
    "    duplicates (all except for the last occurrence) and columns with NA are removed.'''\n",
    "    df = data.copy()\n",
    "    df.drop_duplicates(subset=key_columns, keep='last', inplace=True, ignore_index=True)\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_data = drop_duplicates_na(author_data, ['author_ids'])\n",
    "comments_data = drop_duplicates_na(comment_data, ['comment_ids'])\n",
    "gilds_data = drop_duplicates_na(gilds_data, ['comment_ids'])\n",
    "threads_data = drop_duplicates_na(threads_data, ['thread_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save(df, savepath, filename):\n",
    "    '''Takes in a dataframe and a filename for saving to csv'''\n",
    "    Path(savepath).mkdir(parents=True, exist_ok=True) #Make the folder if it already doesn't exist\n",
    "    df.to_csv(f\"{savepath}{filename}.csv\", header=False)\n",
    "\n",
    "save(authors_data, \"../data/interim/\", \"author_data\")\n",
    "save(comments_data, \"../data/interim/\", \"comment_data\")\n",
    "save(gilds_data, \"../data/interim/\", \"gildings_data\")\n",
    "save(threads_data, \"../data/interim/\", \"threads_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "def toDays(x, currentime):\n",
    "    '''Takes in created_utc time (Unix time, in seconds) and current time. Calculates\n",
    "    age and returns the number of days'''\n",
    "    d = timedelta(seconds=currentime-x)\n",
    "    return d.days\n",
    "\n",
    "def process_dates(data, new_column):\n",
    "    '''Convert the created_utc time to datetime, fetch the days and place it\n",
    "    in a new column. Also removes the original created_utc column'''\n",
    "    df = data.copy()\n",
    "    now = time.time() #This will differ everytime, but the change will be constant across all the rows.\n",
    "    df[new_column] = df.created_utc.map(lambda x: toDays(x, now))\n",
    "    return df.drop([\"created_utc\"], axis=1)\n",
    "    \n",
    "authors = process_dates(authors_data, \"acc_age_days\")\n",
    "comments = process_dates(comments_data, \"comment_age_days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline model, we will work with binary classification (whether a comment is gilded or not). For that, we need to do slightly different transformations, and since there's a large imbalance (gilded vs not gilded comments), we will have to transform the full data and then use stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_gilds = gilds_data.merge(comments, how='outer', on='comment_ids')\n",
    "comments_all = comment_gilds.merge(authors, how='inner', on='author_ids')\n",
    "targets = comments_all[\"gildings\"]\n",
    "comments_all = comments_all.filter(items=[\"ups\", \"comment_karma\", \"link_karma\", \n",
    "                                        \"is_premium\", \"comment_age_days\", \"acc_age_days\"], axis=1)\n",
    "targets.fillna(0, inplace=True)\n",
    "targets = pd.DataFrame(targets.apply(lambda x: 1 if x != 0 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splits = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in splits.split(comments_all, targets):\n",
    "    pass\n",
    "X_train = comments_all.iloc[train_index, :]\n",
    "y_train = targets.iloc[train_index, :]\n",
    "X_test = comments_all.iloc[test_index, :]\n",
    "y_test = targets.iloc[test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "test_data = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(train_data, \"../data/processed/\", \"train_data_baseline\")\n",
    "save(test_data, \"../data/processed/\", \"test_data_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(comments_all, vars=[\"gildings\", \"ups\", \"acc_age_days\", \"comment_karma\", \"link_karma\"])\n",
    "g.map(plt.scatter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Programming-Files",
   "language": "python",
   "name": "programming-files"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
