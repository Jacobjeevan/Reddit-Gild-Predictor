{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"functions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datastore import DataStore\n",
    "from searchgrid import SearchGrid\n",
    "from crossvalidate import CrossValidate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sampleddatastore import SampledDataStore as sds\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load object for CrossValidation\n",
    "crossvalidate = CrossValidate()\n",
    "\n",
    "#Load object for GridSearchCV\n",
    "GridSpace = SearchGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.20713950211366838\n",
      "ROC-AUC is: 0.6098990846479329\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, as with previous classifiers, we want to try adjusting the weights. Let's try balanced before moving onto to GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.20095948827292112\n",
      "ROC-AUC is: 0.5940319965611066\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "crossvalidate.setClassifier(classifier)\n",
    "crossvalidate.run()\n",
    "f1, roc = crossvalidate.getMetrics().getScores()\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use GridSearch to find the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.23215108245048366\n",
      "ROC-AUC is: 0.6257549721670259\n",
      "Best Parameters: {'class_weight': {0: 10, 1: 1}}\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "parameters = {'class_weight':[{0:1,1:1}, {0:1,1:10}, {0:1,1:100}, {0:10,1:1}]}\n",
    "GridSpace.setGridParameters(parameters)\n",
    "GridSpace.setClassifier(classifier)\n",
    "GridSpace.run()\n",
    "parameters, scores = GridSpace.getMetrics().getBestResults()\n",
    "f1 = scores[0]\n",
    "roc = scores[1]\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {0: 1, 1: 1}} F1:  0.207 ROC_AUC:  0.61\n",
      "{'class_weight': {0: 1, 1: 10}} F1:  0.211 ROC_AUC:  0.605\n",
      "{'class_weight': {0: 1, 1: 100}} F1:  0.194 ROC_AUC:  0.595\n",
      "{'class_weight': {0: 10, 1: 1}} F1:  0.232 ROC_AUC:  0.626\n"
     ]
    }
   ],
   "source": [
    "params, scores = GridSpace.getMetrics().getAllresults()\n",
    "for i, j in zip(params, scores):\n",
    "    print(i, \"F1: \", round(j[0],3), \"ROC_AUC: \", round(j[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are somewhat suprising, compared to our previous best weights (1:10 for nongilded to gilded). Let's try experimenting with the features next; we will keep both weights for comparison.\n",
    "\n",
    "We will also restrict the max_depth (to avoid overfitting and to limit the resources needed for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is: 0.2825450623940841\n",
      "ROC-AUC is: 0.7286708002744751\n",
      "Best Parameters: {'class_weight': {0: 1, 1: 10}, 'max_depth': 4, 'max_features': 4}\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "parameters = {'max_depth' : [2, 4],\n",
    "    'class_weight':[{0:1,1:10}, {0:10,1:1}], 'max_features' : [2, 4, 6, 8]}\n",
    "GridSpace.setGridParameters(parameters)\n",
    "GridSpace.setClassifier(classifier)\n",
    "GridSpace.run()\n",
    "GridSpace.save(\"DTGrid01\") #Saving the grid as well, we can reuse it later\n",
    "parameters, scores = GridSpace.getMetrics().getBestResults()\n",
    "f1 = scores[0]\n",
    "roc = scores[1]\n",
    "print(f\"F1 score is: {f1}\")\n",
    "print(f\"ROC-AUC is: {roc}\")\n",
    "print(f\"Best Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Scores:\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 2, 'max_features': 2} F1:  0.243 ROC_AUC:  0.757\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 2, 'max_features': 4} F1:  0.243 ROC_AUC:  0.757\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 2, 'max_features': 6} F1:  0.253 ROC_AUC:  0.724\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 2, 'max_features': 8} F1:  0.253 ROC_AUC:  0.724\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 4, 'max_features': 2} F1:  0.278 ROC_AUC:  0.72\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 4, 'max_features': 4} F1:  0.283 ROC_AUC:  0.729\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 4, 'max_features': 6} F1:  0.278 ROC_AUC:  0.735\n",
      "{'class_weight': {0: 1, 1: 10}, 'max_depth': 4, 'max_features': 8} F1:  0.281 ROC_AUC:  0.737\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 2, 'max_features': 2} F1:  0.0 ROC_AUC:  0.5\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 2, 'max_features': 4} F1:  0.0 ROC_AUC:  0.5\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 2, 'max_features': 6} F1:  0.0 ROC_AUC:  0.5\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 2, 'max_features': 8} F1:  0.0 ROC_AUC:  0.5\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 4, 'max_features': 2} F1:  0.005 ROC_AUC:  0.501\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 4, 'max_features': 4} F1:  0.034 ROC_AUC:  0.509\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 4, 'max_features': 6} F1:  0.044 ROC_AUC:  0.511\n",
      "{'class_weight': {0: 10, 1: 1}, 'max_depth': 4, 'max_features': 8} F1:  0.049 ROC_AUC:  0.513\n"
     ]
    }
   ],
   "source": [
    "params, scores = GridSpace.getAllresults()\n",
    "print(\"\\nAll Scores:\")\n",
    "for i, j in zip(params, scores):\n",
    "    print(i, \"F1: \", round(j[0],3), \"ROC_AUC: \", round(j[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like limiting the tree depth has produced results similar to our previous models. We can use graphviz to visualize the results (saved as tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4, class_weight={0:1, 1:10}, max_features=4)\n",
    "clf = clf.fit(Data.getxTrain(), Data.getyTrain())\n",
    "\n",
    "features_list=[\"ups\", \"comment_karma\", \"link_karma\", \"is_premium\", \"comment_age_days\", \"acc_age_days\", \"comment_length\", \n",
    "                 \"account_activity\"]\n",
    "tree.export_graphviz(clf, out_file='tree.dot', \n",
    "                     feature_names=features_list,  \n",
    "                      class_names=['Non gilded', 'gilded'],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)\n",
    "!dot -Tpng tree.dot -o tree.png #Convert dot file into png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best parameters, we can evaluate the results on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.288021534320323\n",
      "ROC_AUC score: 0.7133447487906208\n",
      "Balanced accuracy score: 0.7133447487906208\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GridSpace = GridSpace.loadGrid(\"models/DTGrid01.pkl\")\n",
    "y_preds = GridSpace.grid.predict(Data.getxTest())\n",
    "print(f\"F1 score: {f1_score(Data.getyTest(), y_preds)}\")\n",
    "print(f\"ROC_AUC score: {roc_auc_score(Data.getyTest(), y_preds)}\")\n",
    "print(f\"Balanced accuracy score: {balanced_accuracy_score(Data.getyTest(), y_preds)}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable results to both SVM (0.29) and Logistic Regression (0.297), before sampling. Let's apply resampling techniques, train the model and then evaluate them on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sampling Data\n"
     ]
    }
   ],
   "source": [
    "SampledDataStore = sds()\n",
    "SampledDataStore.initializeSamplers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OverSampler on test set:\n",
      "F1 score: 0.007978623311128674\n",
      "ROC_AUC score: 0.7195133303097084\n",
      "Balanced accuracy score: 0.7195133303097084\n",
      "\n",
      "\n",
      "SMOTE on test set:\n",
      "F1 score: 0.030641466208476516\n",
      "ROC_AUC score: 0.8768613188870834\n",
      "Balanced accuracy score: 0.8768613188870835\n",
      "\n",
      "\n",
      "ADASYN on test set:\n",
      "F1 score: 0.03624519792884583\n",
      "ROC_AUC score: 0.8907015234222002\n",
      "Balanced accuracy score: 0.8907015234222002\n",
      "\n",
      "\n",
      "SMOTE TOMEK on test set:\n",
      "F1 score: 0.043701799485861184\n",
      "ROC_AUC score: 0.9060292080658542\n",
      "Balanced accuracy score: 0.9060292080658543\n",
      "\n",
      "\n",
      "SMOTE ENN on test set:\n",
      "F1 score: 0.030482870245481523\n",
      "ROC_AUC score: 0.8976386960612805\n",
      "Balanced accuracy score: 0.8976386960612805\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random = SampledDataStore.getRandomSampled\n",
    "smote = SampledDataStore.getSMOTESampled\n",
    "ada = SampledDataStore.getADASYNSampled\n",
    "smote_tomek = SampledDataStore.getSMOTETOMEKSampled\n",
    "smote_enn = SampledDataStore.getSMOTEENNSampled\n",
    "samplers = [random, smote, ada, smote_tomek, smote_enn]\n",
    "sampler_names = [\"Random OverSampler\", \"SMOTE\", \"ADASYN\", \"SMOTE TOMEK\", \"SMOTE ENN\"]\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=4, max_features=4)\n",
    "\n",
    "for i in range(len(samplers)):\n",
    "    parameters = {'class_weight':[{0:1,1:10}]}\n",
    "    X_resampled, y_resampled = samplers[i]()\n",
    "    GridSpace.getDataStore().setxTrain(X_resampled)\n",
    "    GridSpace.getDataStore().setyTrain(y_resampled) \n",
    "    GridSpace.setGridParameters(parameters)\n",
    "    GridSpace.setClassifier(classifier)\n",
    "    grid = GridSpace.run()\n",
    "    GridSpace.save(f\"DT_{sampler_names[i]}\")\n",
    "    y_preds = grid.predict(GridSpace.getDataStore().getxTest())\n",
    "    print(f\"{sampler_names[i]} on test set:\")\n",
    "    print(f\"F1 score: {f1_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"ROC_AUC score: {roc_auc_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(f\"Balanced accuracy score: {balanced_accuracy_score(GridSpace.getDataStore().getyTest(), y_preds)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "GridSpace.getDataStore().revertToOriginal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, results are worse when compared to both Logistic Regression and SVM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
