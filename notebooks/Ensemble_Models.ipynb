{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try applying Ensemble learning techniques (Bagging and Boosting) on the reddit data.\n",
    "\n",
    "Both bagging and boosting involves training multiple classifiers of the same variety on random subsamples of the data. Bagging trains multiple classifiers in parallel (scales well) while boosting does so in sequential order. This allows boosting to assign weights to samples (i.e. assign a higher weight to samples that were misclassified by the previous learner, thereby iteratively improving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/processed/train_data_baseline.csv\")\n",
    "test_data = pd.read_csv(\"../data/processed/test_data_baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_add_attributes(BaseEstimator, TransformerMixin):\n",
    "    '''Custom transformer based on Sklearn's classes.\n",
    "    Takes in dataframe (train or test) and adds new features and returns\n",
    "    a filtered version of the original train/test datasets.'''\n",
    "    def fit(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return self.fit_transform(X)\n",
    "    def fit_transform(self, X, y=None):\n",
    "        '''Calculates and adds comment body length and account activity (based on frequency of comment author)\n",
    "        as features. Returns a new dataframe with the added columns.'''\n",
    "        data = X.copy()\n",
    "        data[\"body_len\"] = data.comment_body.apply(lambda x: len(x))\n",
    "        data[\"acc_activity\"] = data.author_ids.map(data.author_ids.value_counts())\n",
    "        data[\"is_premium\"] = data.is_premium.astype(int)\n",
    "        return data.filter(items=[\"ups\", \"comment_karma\", \"link_karma\", \"is_premium\", \"comment_age_days\", \"acc_age_days\", \"body_len\", \"acc_activity\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('filter_add', filter_add_attributes()),\n",
    "        ('scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train = pipeline.fit_transform(train_data)\n",
    "X_test = pipeline.transform(test_data)\n",
    "y_train = train_data[\"gildings\"].to_list()\n",
    "y_test = test_data[\"gildings\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(clf, X, y):\n",
    "    '''Takes in train and train datasets along a model to train and evaluate. Returns f1 and roc-auc scores.'''\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
    "    scores = cross_validate(clf, X, y, cv=rskf, scoring=['f1', 'roc_auc'])\n",
    "    return scores['test_f1'], scores['test_roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learn also provides us with variations of regular ensemble models (BalancedBaggingClassifier for BaggingClassifier, BalancedRandomForestClassifier for RandomForstClassifier etc). First, we will run both versions with default hyperparameters to serve as baseline models.\n",
    "\n",
    "Balanced bagging classifier uses RandomUnderSampler to undersample the training set before applying bagging classifier. We use sampling strategy 'auto', which results in resampling (under sampling) only the majority class (class 0 in our case). BalancedRF classifier performs the same operation before applying RF classifier.\n",
    "\n",
    "Read more:https://imbalanced-learn.readthedocs.io/en/stable/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "balanced_bagging = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10,\n",
    "                                             sampling_strategy='auto',  \n",
    "                                             replacement=False, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "models = [bagging, balanced_bagging, rf, brf]\n",
    "model_names = [\"Bagging\", \"Balanced Bagging\", \"Random Forests\", \"Balanced Random Forests\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    f1, roc = train_eval(models[i], X_train, y_train)\n",
    "    print(f\"{model_names[i]}:\")\n",
    "    print(f\"F1 score is: {np.mean(f1)}\")\n",
    "    print(f\"ROC-AUC is: {np.mean(roc)}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced Learn also offers two variations of AdaBoost. We will also run Sklearn's Adaboost for comparison.\n",
    "\n",
    "RusBoost integrates undersampling into AdaBoost, while EasyEnsembleClassifer is a bag of balanced boosted (Adaboost) learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(n_estimators=200, algorithm='SAMME.R', random_state=42)\n",
    "rusboost = RUSBoostClassifier(n_estimators=200, algorithm='SAMME.R', random_state=42)\n",
    "eec = EasyEnsembleClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "models = [adaboost, rusboost, eec]\n",
    "model_names = [\"AdaBoost\", \"RusBoost\", \"Easy Ensemble Boost\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    f1, roc = train_eval(models[i], X_train, y_train)\n",
    "    print(f\"{model_names[i]}:\")\n",
    "    print(f\"F1 score is: {np.mean(f1)}\")\n",
    "    print(f\"ROC-AUC is: {np.mean(roc)}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Programming-Files",
   "language": "python",
   "name": "programming-files"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
